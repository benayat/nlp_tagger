main-pos-simple:
27 epochs
=== Training: ===
Fine 01 | train 2.16134 / 49.05% | dev 0.86794 / 76.80% | lr 1.0e-02
Fine 02 | train 0.47734 / 87.91% | dev 0.27194 / 92.91% | lr 1.0e-02
Fine 03 | train 0.24414 / 93.78% | dev 0.22143 / 93.99% | lr 1.0e-02
Fine 04 | train 0.21086 / 94.50% | dev 0.20226 / 94.42% | lr 1.0e-02
Fine 05 | train 0.19754 / 94.77% | dev 0.19722 / 94.37% | lr 1.0e-02
Fine 06 | train 0.18983 / 94.95% | dev 0.19196 / 94.56% | lr 1.0e-02
Fine 07 | train 0.18499 / 95.05% | dev 0.18995 / 94.58% | lr 1.0e-02
Fine 08 | train 0.18053 / 95.15% | dev 0.18564 / 94.78% | lr 1.0e-02
Fine 09 | train 0.17729 / 95.22% | dev 0.18244 / 94.77% | lr 1.0e-02
Fine 10 | train 0.17526 / 95.29% | dev 0.18212 / 94.97% | lr 1.0e-02
Fine 11 | train 0.17174 / 95.35% | dev 0.18201 / 94.79% | lr 1.0e-02
Fine 12 | train 0.17038 / 95.41% | dev 0.18013 / 94.87% | lr 1.0e-02
Fine 13 | train 0.16861 / 95.46% | dev 0.17919 / 94.86% | lr 1.0e-02
Fine 14 | train 0.16666 / 95.51% | dev 0.17800 / 94.83% | lr 1.0e-02
Fine 15 | train 0.16467 / 95.58% | dev 0.17498 / 95.01% | lr 1.0e-02
Fine 16 | train 0.16301 / 95.63% | dev 0.17412 / 95.03% | lr 1.0e-02
Fine 17 | train 0.16139 / 95.65% | dev 0.17668 / 94.92% | lr 1.0e-02
Learning-rate decayed → 1.0e-03; weight-decay scaled × 0.100
Fine 18 | train 0.16078 / 95.70% | dev 0.17629 / 94.98% | lr 1.0e-03
Fine 19 | train 0.12899 / 96.67% | dev 0.15521 / 95.46% | lr 1.0e-03
Fine 20 | train 0.10433 / 97.20% | dev 0.14472 / 95.66% | lr 1.0e-03
Fine 21 | train 0.09168 / 97.45% | dev 0.13990 / 95.72% | lr 1.0e-03
Fine 22 | train 0.08436 / 97.61% | dev 0.13695 / 95.74% | lr 1.0e-03
Fine 23 | train 0.07892 / 97.72% | dev 0.13489 / 95.75% | lr 1.0e-03
Fine 24 | train 0.07529 / 97.79% | dev 0.13357 / 95.79% | lr 1.0e-03
Fine 25 | train 0.07223 / 97.86% | dev 0.13317 / 95.82% | lr 1.0e-03
Fine 26 | train 0.06961 / 97.92% | dev 0.13261 / 95.81% | lr 1.0e-03
Fine 27 | train 0.06726 / 98.01% | dev 0.13238 / 95.82% | lr 1.0e-03

main-ner-simple:
epochs: 13
=== Training: ===
Fine 01 | train 2.13207 / 34.10% | dev 1.06316 / 70.03% | lr 3.0e-02
Fine 02 | train 0.70433 / 80.57% | dev 0.89757 / 73.49% | lr 3.0e-02
Fine 03 | train 0.63332 / 82.31% | dev 0.89597 / 73.39% | lr 3.0e-02
Fine 04 | train 0.53856 / 85.07% | dev 0.82914 / 75.17% | lr 3.0e-02
Fine 05 | train 0.49380 / 86.40% | dev 0.81473 / 76.47% | lr 3.0e-02
Fine 06 | train 0.43614 / 88.12% | dev 0.81203 / 75.83% | lr 3.0e-02
Fine 07 | train 0.39236 / 89.18% | dev 0.79905 / 76.50% | lr 3.0e-02
Fine 08 | train 0.35870 / 90.35% | dev 0.80940 / 76.23% | lr 3.0e-02
Learning-rate decayed → 3.0e-03; weight-decay scaled × 0.100
Fine 09 | train 0.33078 / 90.91% | dev 0.81527 / 75.90% | lr 3.0e-03
Fine 10 | train 0.17657 / 95.92% | dev 0.74590 / 78.67% | lr 3.0e-03
Fine 11 | train 0.07513 / 98.58% | dev 0.76416 / 79.12% | lr 3.0e-03
Learning-rate decayed → 3.0e-04; weight-decay scaled × 0.100
Fine 12 | train 0.04623 / 99.33% | dev 0.77455 / 79.35% | lr 3.0e-04
Fine 13 | train 0.03604 / 99.55% | dev 0.77629 / 79.40% | lr 3.0e-04

main-pos-simple-pretrained:
=== Training: ===
Fine 01 | train 1.51986 / 61.77% | dev 0.57606 / 84.00% | lr 1.0e-02
Fine 02 | train 0.41048 / 88.83% | dev 0.28320 / 92.28% | lr 1.0e-02
Fine 03 | train 0.26507 / 93.05% | dev 0.23619 / 93.47% | lr 1.0e-02
Fine 04 | train 0.22620 / 94.13% | dev 0.21190 / 94.09% | lr 1.0e-02
Fine 05 | train 0.20493 / 94.66% | dev 0.19830 / 94.41% | lr 1.0e-02
Fine 06 | train 0.19262 / 94.96% | dev 0.19025 / 94.72% | lr 1.0e-02
Fine 07 | train 0.18543 / 95.12% | dev 0.18896 / 94.60% | lr 1.0e-02
Fine 08 | train 0.17982 / 95.25% | dev 0.18421 / 94.88% | lr 1.0e-02
Fine 09 | train 0.17610 / 95.33% | dev 0.18115 / 94.84% | lr 1.0e-02
Fine 10 | train 0.17387 / 95.39% | dev 0.18098 / 94.95% | lr 1.0e-02
Fine 11 | train 0.17057 / 95.45% | dev 0.18155 / 94.85% | lr 1.0e-02
Fine 12 | train 0.16928 / 95.47% | dev 0.17982 / 94.86% | lr 1.0e-02
Fine 13 | train 0.16763 / 95.52% | dev 0.17831 / 94.83% | lr 1.0e-02
Fine 14 | train 0.16564 / 95.59% | dev 0.17571 / 94.93% | lr 1.0e-02
Fine 15 | train 0.16395 / 95.61% | dev 0.17345 / 95.04% | lr 1.0e-02
Fine 16 | train 0.16272 / 95.64% | dev 0.17341 / 95.01% | lr 1.0e-02
Fine 17 | train 0.16161 / 95.66% | dev 0.17570 / 94.89% | lr 1.0e-02
Learning-rate decayed → 1.0e-03; weight-decay scaled × 0.100
Fine 18 | train 0.16079 / 95.73% | dev 0.17449 / 95.05% | lr 1.0e-03
Fine 19 | train 0.13024 / 96.64% | dev 0.15389 / 95.47% | lr 1.0e-03
Fine 20 | train 0.10629 / 97.18% | dev 0.14359 / 95.67% | lr 1.0e-03
Fine 21 | train 0.09410 / 97.41% | dev 0.13863 / 95.72% | lr 1.0e-03
Fine 22 | train 0.08691 / 97.56% | dev 0.13584 / 95.76% | lr 1.0e-03
Fine 23 | train 0.08193 / 97.66% | dev 0.13372 / 95.75% | lr 1.0e-03
Fine 24 | train 0.07827 / 97.73% | dev 0.13221 / 95.80% | lr 1.0e-03
Fine 25 | train 0.07521 / 97.79% | dev 0.13175 / 95.79% | lr 1.0e-03
Fine 26 | train 0.07256 / 97.86% | dev 0.13093 / 95.81% | lr 1.0e-03
Fine 27 | train 0.07067 / 97.91% | dev 0.13034 / 95.79% | lr 1.0e-03
Fine 28 | train 0.06860 / 97.96% | dev 0.12989 / 95.82% | lr 1.0e-03
Fine 29 | train 0.06686 / 98.02% | dev 0.12971 / 95.82% | lr 1.0e-03
Fine 30 | train 0.06553 / 98.04% | dev 0.12961 / 95.81% | lr 1.0e-03
Fine 31 | train 0.06407 / 98.09% | dev 0.12942 / 95.80% | lr 1.0e-03
Fine 32 | train 0.06274 / 98.13% | dev 0.12927 / 95.82% | lr 1.0e-03
Fine 33 | train 0.06150 / 98.18% | dev 0.12966 / 95.82% | lr 1.0e-03
Learning-rate decayed → 1.0e-04; weight-decay scaled × 0.100
Fine 34 | train 0.06043 / 98.20% | dev 0.12948 / 95.84% | lr 1.0e-04
Fine 35 | train 0.05819 / 98.29% | dev 0.12931 / 95.82% | lr 1.0e-04
Fine 36 | train 0.05793 / 98.30% | dev 0.12919 / 95.84% | lr 1.0e-04
Fine 37 | train 0.05748 / 98.30% | dev 0.12921 / 95.85% | lr 1.0e-04
Fine 38 | train 0.05724 / 98.32% | dev 0.12925 / 95.83% | lr 1.0e-04
Fine 39 | train 0.05683 / 98.34% | dev 0.12917 / 95.86% | lr 1.0e-04

main-ner-simple-pretrained:
epochs: 12
=== Training: ===
Fine 01 | train 2.15096 / 30.42% | dev 1.24925 / 62.68% | lr 3.0e-02
Fine 02 | train 0.91190 / 73.95% | dev 0.95996 / 70.58% | lr 3.0e-02
Fine 03 | train 0.69305 / 80.93% | dev 0.92057 / 72.75% | lr 3.0e-02
Fine 04 | train 0.63856 / 82.47% | dev 0.87339 / 73.75% | lr 3.0e-02
Fine 05 | train 0.57584 / 84.27% | dev 0.83090 / 75.83% | lr 3.0e-02
Fine 06 | train 0.51816 / 85.74% | dev 0.79493 / 76.22% | lr 3.0e-02
Fine 07 | train 0.46119 / 87.49% | dev 0.77725 / 76.99% | lr 3.0e-02
Fine 08 | train 0.40975 / 88.94% | dev 0.76688 / 77.01% | lr 3.0e-02
Fine 09 | train 0.37555 / 89.68% | dev 0.78998 / 75.97% | lr 3.0e-02
Learning-rate decayed → 3.0e-03; weight-decay scaled × 0.100
Fine 10 | train 0.34849 / 90.55% | dev 0.79465 / 76.69% | lr 3.0e-03
Fine 11 | train 0.19757 / 95.46% | dev 0.73954 / 79.20% | lr 3.0e-03
Fine 12 | train 0.09361 / 98.30% | dev 0.73506 / 79.45% | lr 3.0e-03

main-pos-subword:
epochs: 27
=== Training: ===
Fine 01 | train 1.75355 / 54.25% | dev 0.89698 / 73.76% | lr 1.0e-02
Fine 02 | train 0.72216 / 78.53% | dev 0.49139 / 85.24% | lr 1.0e-02
Fine 03 | train 0.46408 / 86.03% | dev 0.35757 / 89.20% | lr 1.0e-02
Fine 04 | train 0.34950 / 89.55% | dev 0.28307 / 91.44% | lr 1.0e-02
Fine 05 | train 0.28036 / 91.68% | dev 0.23575 / 92.84% | lr 1.0e-02
Fine 06 | train 0.23430 / 93.09% | dev 0.20487 / 93.81% | lr 1.0e-02
Fine 07 | train 0.20449 / 93.98% | dev 0.18605 / 94.18% | lr 1.0e-02
Fine 08 | train 0.18424 / 94.61% | dev 0.17033 / 94.69% | lr 1.0e-02
Fine 09 | train 0.17086 / 94.97% | dev 0.16256 / 94.89% | lr 1.0e-02
Fine 10 | train 0.16148 / 95.24% | dev 0.15315 / 95.20% | lr 1.0e-02
Fine 11 | train 0.15481 / 95.44% | dev 0.15129 / 95.35% | lr 1.0e-02
Fine 12 | train 0.14958 / 95.58% | dev 0.14628 / 95.42% | lr 1.0e-02
Fine 13 | train 0.14574 / 95.69% | dev 0.14497 / 95.43% | lr 1.0e-02
Fine 14 | train 0.14251 / 95.83% | dev 0.14507 / 95.48% | lr 1.0e-02
Fine 15 | train 0.14036 / 95.86% | dev 0.13820 / 95.78% | lr 1.0e-02
Fine 16 | train 0.13857 / 95.91% | dev 0.13572 / 95.80% | lr 1.0e-02
Fine 17 | train 0.13716 / 95.97% | dev 0.13467 / 95.81% | lr 1.0e-02
Fine 18 | train 0.13557 / 96.01% | dev 0.13659 / 95.78% | lr 1.0e-02
Learning-rate decayed → 1.0e-03; weight-decay scaled × 0.100
Fine 19 | train 0.13431 / 96.04% | dev 0.13523 / 95.77% | lr 1.0e-03
Fine 20 | train 0.11193 / 96.74% | dev 0.11640 / 96.35% | lr 1.0e-03
Fine 21 | train 0.09232 / 97.25% | dev 0.10899 / 96.50% | lr 1.0e-03
Fine 22 | train 0.08323 / 97.50% | dev 0.10569 / 96.49% | lr 1.0e-03
Fine 23 | train 0.07774 / 97.64% | dev 0.10376 / 96.61% | lr 1.0e-03
Fine 24 | train 0.07365 / 97.75% | dev 0.10277 / 96.61% | lr 1.0e-03
Fine 25 | train 0.07028 / 97.85% | dev 0.10216 / 96.59% | lr 1.0e-03
Fine 26 | train 0.06790 / 97.92% | dev 0.10117 / 96.67% | lr 1.0e-03
Fine 27 | train 0.06574 / 97.96% | dev 0.10087 / 96.69% | lr 1.0e-03

main-ner-subword: 16 epochs
=== Training: ===
Fine 01 | train 2.82028 / 15.38% | dev 2.18900 / 31.73% | lr 3.0e-02
Fine 02 | train 1.65334 / 48.74% | dev 1.21174 / 64.22% | lr 3.0e-02
Fine 03 | train 0.95277 / 71.83% | dev 0.94242 / 72.41% | lr 3.0e-02
Fine 04 | train 0.74693 / 78.25% | dev 0.87344 / 74.78% | lr 3.0e-02
Fine 05 | train 0.67314 / 80.42% | dev 0.80665 / 76.90% | lr 3.0e-02
Fine 06 | train 0.61477 / 82.49% | dev 0.84439 / 74.98% | lr 3.0e-02
Fine 07 | train 0.56491 / 83.93% | dev 0.80146 / 76.85% | lr 3.0e-02
Fine 08 | train 0.52340 / 84.82% | dev 0.79933 / 76.91% | lr 3.0e-02
Fine 09 | train 0.50193 / 85.67% | dev 0.75366 / 78.68% | lr 3.0e-02
Fine 10 | train 0.46709 / 86.40% | dev 0.78843 / 77.86% | lr 3.0e-02
Learning-rate decayed → 3.0e-03; weight-decay scaled × 0.100
Fine 11 | train 0.46142 / 86.63% | dev 0.77540 / 78.01% | lr 3.0e-03
Fine 12 | train 0.25731 / 92.89% | dev 0.62693 / 83.24% | lr 3.0e-03
Fine 13 | train 0.11606 / 97.53% | dev 0.60722 / 84.09% | lr 3.0e-03
Fine 14 | train 0.07126 / 98.82% | dev 0.61951 / 83.91% | lr 3.0e-03
Learning-rate decayed → 3.0e-04; weight-decay scaled × 0.100
Fine 15 | train 0.05356 / 99.23% | dev 0.62175 / 84.01% | lr 3.0e-04
Fine 16 | train 0.04186 / 99.54% | dev 0.62739 / 84.08% | lr 3.0e-04

main-pos-subword-pretrained:
epochs: 35
=== Training: ===
Fine 01 | train 1.65089 / 56.85% | dev 0.79987 / 76.42% | lr 1.0e-02
Fine 02 | train 0.66171 / 80.23% | dev 0.45972 / 86.23% | lr 1.0e-02
Fine 03 | train 0.44402 / 86.66% | dev 0.34324 / 89.61% | lr 1.0e-02
Fine 04 | train 0.34303 / 89.74% | dev 0.27474 / 91.70% | lr 1.0e-02
Fine 05 | train 0.27635 / 91.78% | dev 0.23185 / 92.86% | lr 1.0e-02
Fine 06 | train 0.23138 / 93.19% | dev 0.20156 / 93.94% | lr 1.0e-02
Fine 07 | train 0.20082 / 94.13% | dev 0.18204 / 94.44% | lr 1.0e-02
Fine 08 | train 0.18096 / 94.69% | dev 0.16730 / 94.92% | lr 1.0e-02
Fine 09 | train 0.16709 / 95.11% | dev 0.15821 / 95.12% | lr 1.0e-02
Fine 10 | train 0.15765 / 95.35% | dev 0.15062 / 95.43% | lr 1.0e-02
Fine 11 | train 0.15137 / 95.53% | dev 0.14537 / 95.65% | lr 1.0e-02
Fine 12 | train 0.14653 / 95.68% | dev 0.14140 / 95.64% | lr 1.0e-02
Fine 13 | train 0.14277 / 95.80% | dev 0.14066 / 95.61% | lr 1.0e-02
Fine 14 | train 0.13992 / 95.89% | dev 0.13870 / 95.75% | lr 1.0e-02
Fine 15 | train 0.13804 / 95.94% | dev 0.13647 / 95.85% | lr 1.0e-02
Fine 16 | train 0.13619 / 96.00% | dev 0.13561 / 95.91% | lr 1.0e-02
Fine 17 | train 0.13467 / 96.03% | dev 0.13467 / 95.89% | lr 1.0e-02
Fine 18 | train 0.13363 / 96.06% | dev 0.13518 / 95.69% | lr 1.0e-02
Fine 19 | train 0.13257 / 96.09% | dev 0.13402 / 95.81% | lr 1.0e-02
Fine 20 | train 0.13159 / 96.11% | dev 0.13406 / 95.86% | lr 1.0e-02
Fine 21 | train 0.13107 / 96.16% | dev 0.13196 / 95.99% | lr 1.0e-02
Fine 22 | train 0.13058 / 96.16% | dev 0.13208 / 95.94% | lr 1.0e-02
Learning-rate decayed → 1.0e-03; weight-decay scaled × 0.100
Fine 23 | train 0.12944 / 96.19% | dev 0.13366 / 95.90% | lr 1.0e-03
Fine 24 | train 0.10864 / 96.85% | dev 0.11611 / 96.37% | lr 1.0e-03
Fine 25 | train 0.09121 / 97.30% | dev 0.10941 / 96.46% | lr 1.0e-03
Fine 26 | train 0.08289 / 97.52% | dev 0.10652 / 96.54% | lr 1.0e-03
Fine 27 | train 0.07741 / 97.65% | dev 0.10475 / 96.59% | lr 1.0e-03
Fine 28 | train 0.07319 / 97.77% | dev 0.10320 / 96.62% | lr 1.0e-03
Fine 29 | train 0.07030 / 97.85% | dev 0.10217 / 96.67% | lr 1.0e-03
Fine 30 | train 0.06789 / 97.91% | dev 0.10177 / 96.62% | lr 1.0e-03
Fine 31 | train 0.06574 / 97.96% | dev 0.10139 / 96.69% | lr 1.0e-03
Fine 32 | train 0.06392 / 98.02% | dev 0.10139 / 96.69% | lr 1.0e-03
Fine 33 | train 0.06236 / 98.05% | dev 0.10114 / 96.68% | lr 1.0e-03
Fine 34 | train 0.06070 / 98.11% | dev 0.10115 / 96.65% | lr 1.0e-03
Learning-rate decayed → 1.0e-04; weight-decay scaled × 0.100
Fine 35 | train 0.05961 / 98.13% | dev 0.10118 / 96.71% | lr 1.0e-04

main-ner-subword-pretrained:
epochs: 14
=== Training: ===
Fine 01 | train 2.82726 / 15.93% | dev 2.17856 / 31.96% | lr 3.0e-02
Fine 02 | train 1.63924 / 49.25% | dev 1.19861 / 65.18% | lr 3.0e-02
Fine 03 | train 0.92857 / 72.85% | dev 0.92764 / 72.71% | lr 3.0e-02
Fine 04 | train 0.72086 / 79.13% | dev 0.83140 / 75.41% | lr 3.0e-02
Fine 05 | train 0.64813 / 81.32% | dev 0.82484 / 75.99% | lr 3.0e-02
Fine 06 | train 0.59902 / 82.96% | dev 0.83045 / 75.85% | lr 3.0e-02
Fine 07 | train 0.56800 / 83.69% | dev 0.79066 / 77.72% | lr 3.0e-02
Fine 08 | train 0.51881 / 85.08% | dev 0.77052 / 78.05% | lr 3.0e-02
Fine 09 | train 0.49445 / 85.65% | dev 0.74331 / 79.01% | lr 3.0e-02
Fine 10 | train 0.46783 / 86.50% | dev 0.75023 / 78.56% | lr 3.0e-02
Learning-rate decayed → 3.0e-03; weight-decay scaled × 0.100
Fine 11 | train 0.45076 / 87.07% | dev 0.74409 / 79.28% | lr 3.0e-03
Fine 12 | train 0.26026 / 92.89% | dev 0.62382 / 82.98% | lr 3.0e-03
Fine 13 | train 0.12019 / 97.37% | dev 0.62067 / 83.50% | lr 3.0e-03
Fine 14 | train 0.07570 / 98.65% | dev 0.62194 / 83.91% | lr 3.0e-03

main-pos-charcnn:
epochs: 25
=== Training: ===
Fine 01 | train 2.18696 / 42.02% | dev 1.00860 / 74.58% | lr 1.0e-02
Fine 02 | train 0.60369 / 85.25% | dev 0.26573 / 93.40% | lr 1.0e-02
Fine 03 | train 0.25145 / 93.86% | dev 0.18155 / 95.20% | lr 1.0e-02
Fine 04 | train 0.19801 / 94.93% | dev 0.15872 / 95.51% | lr 1.0e-02
Fine 05 | train 0.18026 / 95.24% | dev 0.14864 / 95.79% | lr 1.0e-02
Fine 06 | train 0.16973 / 95.43% | dev 0.14325 / 95.79% | lr 1.0e-02
Fine 07 | train 0.16241 / 95.55% | dev 0.13864 / 95.90% | lr 1.0e-02
Fine 08 | train 0.15702 / 95.67% | dev 0.13678 / 95.93% | lr 1.0e-02
Fine 09 | train 0.15374 / 95.71% | dev 0.13450 / 96.07% | lr 1.0e-02
Fine 10 | train 0.15111 / 95.75% | dev 0.13343 / 96.08% | lr 1.0e-02
Fine 11 | train 0.14821 / 95.81% | dev 0.13091 / 96.19% | lr 1.0e-02
Fine 12 | train 0.14577 / 95.87% | dev 0.12956 / 96.08% | lr 1.0e-02
Fine 13 | train 0.14370 / 95.91% | dev 0.12748 / 96.16% | lr 1.0e-02
Fine 14 | train 0.14263 / 95.92% | dev 0.12890 / 96.05% | lr 1.0e-02
Learning-rate decayed → 1.0e-03; weight-decay scaled × 0.100
Fine 15 | train 0.14093 / 95.96% | dev 0.12832 / 96.12% | lr 1.0e-03
Fine 16 | train 0.12321 / 96.46% | dev 0.11329 / 96.46% | lr 1.0e-03
Fine 17 | train 0.10446 / 96.91% | dev 0.10572 / 96.66% | lr 1.0e-03
Fine 18 | train 0.09481 / 97.13% | dev 0.10198 / 96.69% | lr 1.0e-03
Fine 19 | train 0.08818 / 97.32% | dev 0.10034 / 96.75% | lr 1.0e-03
Fine 20 | train 0.08353 / 97.43% | dev 0.09866 / 96.77% | lr 1.0e-03
Fine 21 | train 0.07991 / 97.54% | dev 0.09847 / 96.77% | lr 1.0e-03
Fine 22 | train 0.07694 / 97.61% | dev 0.09783 / 96.75% | lr 1.0e-03
Fine 23 | train 0.07481 / 97.68% | dev 0.09727 / 96.79% | lr 1.0e-03
Fine 24 | train 0.07229 / 97.76% | dev 0.09743 / 96.81% | lr 1.0e-03
Fine 25 | train 0.07045 / 97.82% | dev 0.09680 / 96.86% | lr 1.0e-03

main-ner-charcnn:
epochs: 20
=== Training: ===
Fine 01 | train 3.55340 /  1.98% | dev 3.99196 /  0.00% | lr 3.0e-02
Fine 02 | train 2.87004 /  7.56% | dev 1.70737 / 30.90% | lr 3.0e-02
Fine 03 | train 1.86653 / 26.06% | dev 1.48192 / 46.94% | lr 3.0e-02
Fine 04 | train 1.38740 / 50.53% | dev 0.95763 / 68.95% | lr 3.0e-02
Fine 05 | train 0.80619 / 75.81% | dev 0.68387 / 78.88% | lr 3.0e-02
Fine 06 | train 0.62656 / 81.83% | dev 0.61851 / 80.91% | lr 3.0e-02
Fine 07 | train 0.55968 / 83.78% | dev 0.60022 / 81.22% | lr 3.0e-02
Fine 08 | train 0.49783 / 85.84% | dev 0.57046 / 83.23% | lr 3.0e-02
Fine 09 | train 0.46543 / 86.76% | dev 0.53345 / 84.18% | lr 3.0e-02
Fine 10 | train 0.42349 / 88.16% | dev 0.55870 / 82.97% | lr 3.0e-02
Fine 11 | train 0.40067 / 88.75% | dev 0.53074 / 84.32% | lr 3.0e-02
Fine 12 | train 0.37254 / 89.66% | dev 0.52253 / 84.41% | lr 3.0e-02
Fine 13 | train 0.35880 / 89.96% | dev 0.51921 / 84.66% | lr 3.0e-02
Fine 14 | train 0.34790 / 90.46% | dev 0.51408 / 84.82% | lr 3.0e-02
Fine 15 | train 0.33532 / 90.67% | dev 0.51547 / 85.54% | lr 3.0e-02
Learning-rate decayed → 3.0e-03; weight-decay scaled × 0.100
Fine 16 | train 0.32404 / 91.12% | dev 0.52922 / 84.11% | lr 3.0e-03
Fine 17 | train 0.19524 / 95.21% | dev 0.42649 / 87.55% | lr 3.0e-03
Fine 18 | train 0.09450 / 98.13% | dev 0.41915 / 88.42% | lr 3.0e-03
Fine 19 | train 0.05727 / 99.07% | dev 0.42475 / 88.52% | lr 3.0e-03
Learning-rate decayed → 3.0e-04; weight-decay scaled × 0.100
Fine 20 | train 0.04155 / 99.40% | dev 0.42603 / 88.62% | lr 3.0e-04

5gram-cnn-2conv layers with residual:
epochs: 21
epoch 1: train-loss 1.86443 | val loss 1.6196 | perplexity 5.0510 | learning-rate 3.0e-03
epoch 2: train-loss 1.56051 | val loss 1.5366 | perplexity 4.6488 | learning-rate 3.0e-03
epoch 3: train-loss 1.50127 | val loss 1.4998 | perplexity 4.4807 | learning-rate 3.0e-03
epoch 4: train-loss 1.46981 | val loss 1.4857 | perplexity 4.4181 | learning-rate 3.0e-03
Best thee my shall be at:
If the raints?

CLADY:
Chere;
Unoble to the sovereign
his heir know'st to my lord?

EPHGNORTER:
Mark too hand, Seament by.
Gland-sence,
Against time!
Where of the Caplist to out t
epoch 5: train-loss 1.44770 | val loss 1.4668 | perplexity 4.3354 | learning-rate 3.0e-03
epoch 6: train-loss 1.47762 | val loss 1.4605 | perplexity 4.3080 | learning-rate 3.0e-03
epoch 7: train-loss 1.42063 | val loss 1.4503 | perplexity 4.2644 | learning-rate 3.0e-03
epoch 8: train-loss 1.40976 | val loss 1.4450 | perplexity 4.2421 | learning-rate 3.0e-03
epoch 9: train-loss 1.40027 | val loss 1.4411 | perplexity 4.2255 | learning-rate 3.0e-03
Best excelling.

First Serve were you things, all the loss,
I saw, sir?

Third now thou a sir, if it not girll in this time
Think
He that which he before hear,
The receiverse is. Prithee, lest thou pleased
epoch 10: train-loss 1.39315 | val loss 1.4355 | perplexity 4.2015 | learning-rate 3.0e-03
epoch 11: train-loss 1.43159 | val loss 1.4320 | perplexity 4.1870 | learning-rate 3.0e-03
epoch 12: train-loss 1.38156 | val loss 1.4288 | perplexity 4.1735 | learning-rate 3.0e-03
epoch 13: train-loss 1.37491 | val loss 1.4281 | perplexity 4.1707 | learning-rate 3.0e-03
epoch 14: train-loss 1.37032 | val loss 1.4282 | perplexity 4.1711 | learning-rate 3.0e-03
Best he sex your hearted Nerog sea
First Cupid't shunt themations his business.

Second Citizen:
Well, 'pardon.

GLOUCESTER:
Carrosciancertainment, for thinks, why -cure's eyes as him to a boy lays he chee
epoch 15: train-loss 1.36586 | val loss 1.4253 | perplexity 4.1590 | learning-rate 3.0e-03
epoch 16: train-loss 1.40762 | val loss 1.4225 | perplexity 4.1475 | learning-rate 3.0e-03
epoch 17: train-loss 1.35844 | val loss 1.4240 | perplexity 4.1537 | learning-rate 3.0e-03
Learning-rate decayed → 3.0e-04; weight-decay scaled × 0.100
epoch 18: train-loss 1.35440 | val loss 1.4238 | perplexity 4.1530 | learning-rate 3.0e-04
epoch 19: train-loss 1.30040 | val loss 1.3856 | perplexity 3.9971 | learning-rate 3.0e-04
Best of they us and pays.

BENVOLIO:
Not the noble hoxt
should be done took intented
To my moveables in me, unseveral that my slay
Thy father's to me in the fair Petruchious; and yet we have loathed you st
epoch 20: train-loss 1.28538 | val loss 1.3834 | perplexity 3.9883 | learning-rate 3.0e-04
epoch 21: train-loss 1.32710 | val loss 1.3809 | perplexity 3.9785 | learning-rate 3.0e-04